============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
2025-05-08 22:33:09.244053: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-08 22:33:09.355647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1746736389.397463 1670358 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1746736389.412211 1670358 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1746736389.489236 1670358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746736389.489272 1670358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746736389.489275 1670358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1746736389.489277 1670358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-08 22:33:09.497052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cmpatino-uva (erwin-nsa) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /gpfs/home5/scur2687/erwin/experiments/wandb/run-20250508_223317-k4s8wi9l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run erwin_nsa_shapenet
wandb: ‚≠êÔ∏è View project at https://wandb.ai/erwin-nsa/ballformer
wandb: üöÄ View run at https://wandb.ai/erwin-nsa/ballformer/runs/k4s8wi9l
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
input to NSA shape: torch.Size([8192, 64])
output from NSA shape: torch.Size([8192, 64])
Traceback (most recent call last):
  File "/gpfs/home5/scur2687/erwin/experiments/train_shapenet.py", line 162, in <module>
    fit(config, model, optimizer, scheduler, train_loader, valid_loader, test_loader, 110, 160)
  File "/gpfs/home5/scur2687/erwin/experiments/../../erwin/training.py", line 212, in fit
    stat_dict = train_step(model, batch, optimizer, scheduler)
  File "/gpfs/home5/scur2687/erwin/experiments/../../erwin/training.py", line 66, in train_step
    stat_dict["train/loss"].backward()
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 39.50 GiB of which 9.64 GiB is free. Including non-PyTorch memory, this process has 29.85 GiB memory in use. Of the allocated memory 29.25 GiB is allocated by PyTorch, and 104.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/gpfs/home5/scur2687/erwin/experiments/train_shapenet.py", line 162, in <module>
    fit(config, model, optimizer, scheduler, train_loader, valid_loader, test_loader, 110, 160)
  File "/gpfs/home5/scur2687/erwin/experiments/../../erwin/training.py", line 212, in fit
    stat_dict = train_step(model, batch, optimizer, scheduler)
  File "/gpfs/home5/scur2687/erwin/experiments/../../erwin/training.py", line 66, in train_step
    stat_dict["train/loss"].backward()
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/gpfs/home5/scur2687/erwin/erwin/lib64/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 39.50 GiB of which 9.64 GiB is free. Including non-PyTorch memory, this process has 29.85 GiB memory in use. Of the allocated memory 29.25 GiB is allocated by PyTorch, and 104.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33merwin_nsa_shapenet[0m at: [34mhttps://wandb.ai/erwin-nsa/ballformer/runs/k4s8wi9l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_223317-k4s8wi9l/logs[0m
srun: error: gcn11: task 0: Exited with exit code 1
srun: Terminating StepId=11661285.0

JOB STATISTICS
==============
Job ID: 11661285
Cluster: snellius
User/Group: scur2687/scur2687
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:18
CPU Efficiency: 2.70% of 00:11:06 core-walltime
Job Wall-clock time: 00:00:37
Memory Utilized: 1.59 MB
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
